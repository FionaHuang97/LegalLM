{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_single_issue_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6sToZFMdzXQfRGaFLmOnk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charleslow-cmu/bva-capstone/blob/charles-regex/clean_single_issue_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F749NifkLkoS",
        "colab_type": "text"
      },
      "source": [
        "This notebook downloads raw documents from the GCP bucket `bva-appeals-raw-data` and filters it to single-issue documents only. It then writes this smaller set of data in `.avro` format, which is suitable for processing by dask. The processed data is finally uploaded into the GCP bucket `bva-appeals-processed-data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c18I0W4pS2U7",
        "colab_type": "code",
        "outputId": "052e295e-6565-48c1-e5e5-52e7bb8a02b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate to GCP\n",
        "from google.colab import auth\n",
        "project_id = 'bva-appeal'\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project {project_id}\n",
        "raw_bucket_name = \"bva-appeal-raw-data\"\n",
        "processed_bucket_name = \"bva-appeal-processed-data\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGDcL5nkE-Nf",
        "colab_type": "text"
      },
      "source": [
        "This section imports libraries and contains some utility functions. Ideally, these utility functions should be in a common library on the git repo. However, it is not trivial to sync the Colab workflow with the github repository, so currently each Colab notebook is self-contained (i.e. does not import scripts from the github repo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-c-D8vi5IWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# Libraries and Utility Functions\n",
        "!pip install fastavro\n",
        "from dask.distributed import Client, progress\n",
        "import numpy as np\n",
        "import dask.bag as db\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import glob\n",
        "from dask.distributed import Client, LocalCluster\n",
        "import fastavro\n",
        "import pandas as pd\n",
        "\n",
        "# For some reason, Dask does not work well with the inline %timeit function, so use a simple timer\n",
        "class Timer():\n",
        "    def start(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def end(self):\n",
        "        self.end_time = time.time()\n",
        "        print(f\"Time elapsed: {self.end_time - self.start_time:.2f} seconds.\")\n",
        "\n",
        "# Read text into dask bag\n",
        "def load_case_documents(files, npartitions=100):\n",
        "    def load_from_filename(file):\n",
        "        with open(file, errors=\"ignore\", encoding=\"utf-8\") as f:\n",
        "            filename = file.split(\"/\")[-1].split(\".\")[0]                # Get filename between parent_directory/ and .txt\n",
        "            return {\"bva_id\": int(filename), \"text\" : f.read()}\n",
        "    b = db.from_sequence(files, npartitions=npartitions).map(load_from_filename)\n",
        "    return b\n",
        "\n",
        "# Init timer\n",
        "timer = Timer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cZIYKw033zW",
        "colab_type": "code",
        "outputId": "058ead5e-71c0-4cb8-dafd-e22fddafb634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Download all-bva-decisions from GCP and untar it for processing\n",
        "if not \"all-bva-decisions\" in os.listdir():\n",
        "    timer.start()\n",
        "    !gsutil -m cp gs://$raw_bucket_name/all-bva-decisions.tar.gz .\n",
        "    !tar -xf all-bva-decisions.tar.gz\n",
        "    timer.end()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bva-appeal-raw-data/all-bva-decisions.tar.gz...\n",
            "\\ [1/1 files][  4.6 GiB/  4.6 GiB] 100% Done  67.9 MiB/s ETA 00:00:00           \n",
            "Operation completed over 1 objects/4.6 GiB.                                      \n",
            "Time elapsed: 423.72 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM4pu890DeNS",
        "colab_type": "text"
      },
      "source": [
        "Here we identify the files that are single-issue documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1EcUBo9DcoP",
        "colab_type": "code",
        "outputId": "89a82ae4-db09-4906-b5ff-0e3ff09bcb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Some preprocessing on the documents\n",
        "documents = [x.split(\".\")[0] for x in os.listdir(\"all-bva-decisions\")]\n",
        "print(list(filter(lambda x: not str.isdigit(x), documents)))  \n",
        "!mv all-bva-decisions/9221305a.txt all-bva-decisions/9221305.txt  # There is one document with \"a\" appended\n",
        "!rm all-bva-decisions/all-bva-decisions.tar.gz                    # There is another tar.gz inside\n",
        "documents = np.array(os.listdir(\"all-bva-decisions\"))\n",
        "documents_int = np.array([x.split(\".\")[0] for x in documents], dtype=np.int64)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['all-bva-decisions', '9221305a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUAcHQ646d2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "313f17e5-e08e-49b8-e00c-41fcd4d434d8"
      },
      "source": [
        "# Download BVACases.csv and updated_vacols.csv\n",
        "timer.start()\n",
        "if not \"BVACases.csv\" in os.listdir():\n",
        "    !gsutil -m cp gs://$raw_bucket_name/BVACases.csv .\n",
        "if not \"updated_vacols.csv\" in os.listdir():\n",
        "    !gsutil -m cp gs://$raw_bucket_name/updated_vacols.csv .\n",
        "timer.end()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bva-appeal-raw-data/BVACases.csv...\n",
            "\\ [1/1 files][134.7 MiB/134.7 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/134.7 MiB.                                    \n",
            "Copying gs://bva-appeal-raw-data/updated_vacols.csv...\n",
            "| [1/1 files][151.7 MiB/151.7 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/151.7 MiB.                                    \n",
            "Time elapsed: 8.86 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKaSJml2NMUO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c41c603-a29b-45bb-c227-2d9c81aadc90"
      },
      "source": [
        "# Load BVACases and Vacols\n",
        "dict_types = {\"appeal_id\": str, \n",
        "              \"tiread2\": np.int64, \n",
        "              \"issue_id\": str, \n",
        "              \"imgadtm\": str,\n",
        "              \"issdc\": str,\n",
        "              \"issseq\": str,\n",
        "              \"issprog\": str,\n",
        "              \"isscode\": str,\n",
        "              \"isslev2\": str,\n",
        "              \"isslev3\": str,\n",
        "              \"cvdocket\": str,\n",
        "              \"cvdisp\": str,\n",
        "              \"appealed_CAVC\": np.int32,\n",
        "              \"issue_count\": np.int32}\n",
        "\n",
        "bva = pd.read_csv(\"BVACases.csv\", dtype=dict_types)\n",
        "bva = bva.sort_values(\"appeal_id\").reset_index(drop=True)\n",
        "bva.fillna(\"na\", inplace=True)\n",
        "\n",
        "vacols = pd.read_csv(\"updated_vacols.csv\", dtype=dict_types)\n",
        "vacols.columns.values[0] = \"citation_num\"\n",
        "vacols = vacols.sort_values(\"appeal_id\").reset_index(drop=True)\n",
        "vacols.fillna(\"na\", inplace=True)\n",
        "\n",
        "# Check equality between bva and vacols\n",
        "# Yes, all overlapping columns are identical \n",
        "overlapping_cols = list(set(vacols.columns) & set(bva.columns))\n",
        "any_diff = False\n",
        "for col in overlapping_cols:\n",
        "    diff = np.sum(bva[col] != vacols[col])\n",
        "    if diff > 0:\n",
        "        print(f\"{col}: {diff} rows differ.\")\n",
        "        any_diff = True\n",
        "if not any_diff:\n",
        "    print(\"All overlapping columns between vacols and bva are identical.\")\n",
        "\n",
        "# Append issue_count to vacols\n",
        "vacols[\"issue_count\"] = bva[\"issue_count\"]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All overlapping columns between vacols and bva are identical.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7wtXv9wHugy",
        "colab_type": "code",
        "outputId": "52e1a6f7-6f2b-4a39-82bb-cd45c4dff9ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Find set of documents which are single issue\n",
        "single_issue_citations = np.array(vacols[vacols.issue_count == 1].tiread2)\n",
        "single_issue_documents = documents[np.isin(documents_int, single_issue_citations)]\n",
        "print(f\"There are {len(single_issue_documents):,} single issue documents in corpus.\")\n",
        "vacols_single_issue = vacols[vacols.issue_count == 1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 346,915 single issue documents in corpus.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOClzoqNHVBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b56acee-d3cd-4b42-9918-2010a357dac1"
      },
      "source": [
        "# There are some rows with duplicate tiread2\n",
        "# Keep the row with the earlier imgadtm date\n",
        "vacols_single_issue = vacols_single_issue.sort_values([\"tiread2\", \"imgadtm\"])\n",
        "dups = vacols_single_issue[vacols_single_issue.tiread2.duplicated(keep=\"first\")]\n",
        "print(f\"There are {dups.shape[0]} tiread2 values with more than 1 row in metadata.\")\n",
        "\n",
        "non_dups = vacols_single_issue[~vacols_single_issue.tiread2.duplicated(keep=False)]\n",
        "vacols_dedup = pd.concat((non_dups, dups), ignore_index=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 12 tiread2 values with more than 1 row in metadata.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wydCXcGPMVi3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ebd23299-64f8-444c-c299-abd44c003085"
      },
      "source": [
        "# Upload vacols to GCP\n",
        "vacols_dedup.to_csv(\"vacols_processed.csv\", index=False)\n",
        "!gsutil -m cp vacols_processed.csv gs://$processed_bucket_name"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://vacols_processed.csv [Content-Type=text/csv]...\n",
            "-\n",
            "Operation completed over 1 objects/25.2 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsMI_OrIMyKI",
        "colab_type": "text"
      },
      "source": [
        "This section writes all the single-issue BVA decision documents into avro format and uploads to GCP. Avro format is suitable for reading and processing by Dask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-1_GTlUsnEC",
        "colab_type": "code",
        "outputId": "12da8d41-99f7-4b4b-fc4d-cc64375b1e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Start Dask Client\n",
        "cluster = LocalCluster(processes=False, n_workers=12, threads_per_worker=1, diagnostics_port=None)\n",
        "client = Client(cluster)\n",
        "client\n",
        "\n",
        "# Avro Schema for Storing Documents\n",
        "schema = {'name': 'all-bva-decisions',\n",
        "          'namespace': 'Documents',\n",
        "          'doc': 'Full case documents for all BVA decisions',\n",
        "          'type': 'record',\n",
        "          'fields': [{'name': 'text', 'type': 'string'},\n",
        "                     {'name': 'bva_id', 'type': 'int'}]}\n",
        "\n",
        "# Write documents to Avro (compressed format)\n",
        "timer = Timer()\n",
        "timer.start()\n",
        "folder = \"all-bva-decisions\"\n",
        "list_files = [f\"{folder}/{x}\" for x in single_issue_documents]\n",
        "loaded_files = load_case_documents(list_files)\n",
        "!mkdir single-issue-decisions-avro\n",
        "loaded_files.to_avro(\"single-issue-decisions-avro/decisions.*.avro\", schema=schema, codec='deflate')\n",
        "timer.end()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘single-issue-decisions-avro’: File exists\n",
            "Time elapsed: 437.37 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRkX-3U1rezV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload to GCP\n",
        "%%capture\n",
        "!gsutil -m cp -r single-issue-decisions-avro/ gs://$processed_bucket_name"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}