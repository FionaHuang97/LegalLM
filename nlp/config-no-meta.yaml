# paths to caselaw access metadata. can be found in shared Google Drive
citation_dict_fpaths: ['../utils/f3d.jsonl']
# path to folder with full dataset as txt files
case_coll_dir: ../data/case/
# folder to be used to store all preprocessed cases
preprocessed_dir: ../data/preprocessed-cached/
# folder to be used to store all preprocessed test cases
preprocessed_test_dir: ../data/preprocessed-cached/
# text file with training set ids. one id per line
train_ids_fpath: ../utils/updated_train_ids.txt
# text file with test set ids. one id per line
dev_ids_fpath: ../utils/updated_dev_ids.txt
# like above, but smaller for debugging
train_ids_small_fpath: ../utils/train_data_ids_small.txt
# path and the prefix of six test data ids partitions for six-fold evaluation
test_ids_fpath_prefix: ../utils/test_data_ids
# metadata file for cases
meta_fpath:  ../utils/appeals_meta_wscraped.csv
# path for vocabulary cache
cv_path:  ../utils/raw_vocab.pkl
# path for reduced vocabulary cache
cvx_path:  ../utils/thresholded_vocab.pkl
# output directory for logs
output_dir: bilstm-full-idx-no-meta
# path for checkpoint
load_checkpoint_path: ./bilstm-full-idx-no-meta/bilstm/version_0/checkpoints/epoch=7.ckpt


# pretrain_name is used to specify the pretrained tokenizer for both bilstm and roberta; and pretrained model name for roberta
pretrain_name: roberta-base
# model_type is either bilstm or roberta
model_type: bilstm
# mode is either 'train' or 'test'
mode: train
# set training task from
#  1: cit_class_predictions
#    -predict the next citation index that appeared first in forecasting window
#  2: cit_idx_predictions
#    -predict the next citation class(regulations, code, case) that appeared first in forecasting window
#  3: binary_task
#    - whether there will be a citation in the forecasting window
task: cit_idx_predictions

# batch_size of 128 with gradient accumulation step 4 are used for BiLSTM on tesla_P100
# batch_size of 192 with gradient accumulation step 3 are used for RoBERTa on tesla_P100
batch_size: 128
#number of steps to accumulate the gradients and then using the accumulated gradients to compute the variable updates
gradient_accumulation_steps:  4
#learning rate, we used learning rate of 1e-4 for both BiLSTM and RoBERTa
learning_rate: 1e-4

# context_length and forecast window length are for ablations studies, both set to 64 by default
context_length: 64
forecast_length: 64


# whether append metadata before classification
add_case_meta: False
# whether use the year metadata
enable_meta_year: False
# whether use the issue area code metadata
enable_meta_issarea:  False
# whether use the judge metadata
enable_meta_judge: False


# the following two entries are for customize input, make sure to set mode to test to enable this function
# leave unused metadata to be -1, and make sure the format is a string separated by comma
input_meta: '-1,-1,-1'
# if not used, make sure to set input_text as null
input_text: null